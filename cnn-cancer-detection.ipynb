{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Building CNN From Scratch With Keras For Cancer Detection\n***By Kris Smith***","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd\nimport os\nimport cv2\nfrom PIL import Image ## For working with images\nfrom glob import glob \nfrom keras.callbacks import *\nfrom keras.models import *\nfrom keras.layers.normalization import *\nfrom keras.layers.convolutional import *\nfrom keras.layers.core import *\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint(os.listdir(\"../input/histopathologic-cancer-detection\"))\nimport tensorflow as tf\n\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n#############################################################\n\n\n\n\nimport os, cv2\nfrom scipy import stats\n\nfrom tqdm import tqdm_notebook,trange\n\nfrom time import time\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16\n\nfrom keras.callbacks import TensorBoard\n\n# import the necessary packages\nfrom keras.models import Sequential\n# from keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Dense\nfrom keras import backend as K","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-22T19:06:34.541396Z","iopub.execute_input":"2023-05-22T19:06:34.541754Z","iopub.status.idle":"2023-05-22T19:06:38.134333Z","shell.execute_reply.started":"2023-05-22T19:06:34.541724Z","shell.execute_reply":"2023-05-22T19:06:38.133246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n# EDA","metadata":{}},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"print(f'Total Number of Samples in Training Data = {len(os.listdir(\"../input/histopathologic-cancer-detection/train\"))}')\nprint(f'Total Number of Samples in Test Data = {len(os.listdir(\"../input/histopathologic-cancer-detection/test\"))}')","metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:06:38.136295Z","iopub.execute_input":"2023-05-22T19:06:38.137367Z","iopub.status.idle":"2023-05-22T19:06:41.573443Z","shell.execute_reply.started":"2023-05-22T19:06:38.137317Z","shell.execute_reply":"2023-05-22T19:06:41.572346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/histopathologic-cancer-detection/\"\ntrain_path = path + 'train/'\ntest_path = path + 'test/'\n\n### Read data into pandas frames ###\n## Load training and testing file names into frames\ndf = pd.DataFrame({'path': glob(os.path.join(train_path,'*.tif'))}) \ndf_test = pd.DataFrame({'path': glob(os.path.join(test_path,'*.tif'))}) \n\n## Isolate the id from file names and store as id features in seperate column\ndf['id'] = df.path.map(lambda x: x.split('/')[4].split(\".\")[0]) \n\n## Read in training labels\nlabels = pd.read_csv('../input/histopathologic-cancer-detection/train_labels.csv')\n\n## Add Labels to training data \ndf = df.merge(labels, on = \"id\") # merge labels and filepaths\ndf.sample(7)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:06:41.575698Z","iopub.execute_input":"2023-05-22T19:06:41.576727Z","iopub.status.idle":"2023-05-22T19:06:43.561765Z","shell.execute_reply.started":"2023-05-22T19:06:41.576685Z","shell.execute_reply":"2023-05-22T19:06:43.560873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets Inspect Distribution Among The Classes in Training Data","metadata":{}},{"cell_type":"code","source":"labels['label'].value_counts().plot(kind='barh', title='Class Distribution in Training Data')","metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:06:43.564419Z","iopub.execute_input":"2023-05-22T19:06:43.564750Z","iopub.status.idle":"2023-05-22T19:06:43.807426Z","shell.execute_reply.started":"2023-05-22T19:06:43.564720Z","shell.execute_reply":"2023-05-22T19:06:43.806588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inspect Some Samples From Both Positive and Negative Classes","metadata":{}},{"cell_type":"code","source":"## Filter dataframe to get a subset of each class\nclass0 = df[df['label'] == 0]\nclass1 = df[df['label'] == 1]\n\nfor image in range(5):\n    ## Select a random sample from each class\n    sample0 = class0.sample(1)\n    sample1 = class1.sample(1)\n\n    ## Open image files and plot\n    img_path0 = sample0['path'].values[0]\n    img_path1 = sample1['path'].values[0]\n\n    img0 = Image.open(img_path0)\n    img1 = Image.open(img_path1)\n\n    fig, ax = plt.subplots(1, 2, figsize=(5, 2))\n\n    ax[0].imshow(img0)\n    ax[0].set_title('Class 0')\n    ax[0].axis('off')\n\n    ax[1].imshow(img1)\n    ax[1].set_title('Class 1')\n    ax[1].axis('off')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:06:43.808869Z","iopub.execute_input":"2023-05-22T19:06:43.809259Z","iopub.status.idle":"2023-05-22T19:06:44.962582Z","shell.execute_reply.started":"2023-05-22T19:06:43.809227Z","shell.execute_reply":"2023-05-22T19:06:44.961679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*** Inspecting these images I am realizing that I am not a medical professional and therefore I can see no obvious differences between the positive and negative class images. Therefore I will be relying on the labels and the model to distinguish between the two.***","metadata":{}},{"cell_type":"markdown","source":"---\n# Training","metadata":{}},{"cell_type":"markdown","source":"## Split Training Data","metadata":{}},{"cell_type":"code","source":"\n# df = pd.read_csv('../input/histopathologic-cancer-detection/train_labels.csv')\n# df['label'] = df['label'].astype(str)\n# df = df.sample(1000)","metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:06:44.963865Z","iopub.execute_input":"2023-05-22T19:06:44.964378Z","iopub.status.idle":"2023-05-22T19:06:44.968342Z","shell.execute_reply.started":"2023-05-22T19:06:44.964342Z","shell.execute_reply":"2023-05-22T19:06:44.967448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install git+https://github.com/keras-team/keras-preprocessing.git\ndf = pd.read_csv('../input/histopathologic-cancer-detection/train_labels.csv')\n\ndf['label'] = df['label'].astype(str)\n# df = df.sample(640)\n\ndef append_ext(ID):\n    return(ID+\".tif\")\n\n\ndf[\"id\"] = df[\"id\"].apply(append_ext)\n\n\n\ntrain_datagen = ImageDataGenerator(\n       horizontal_flip=True,\n       vertical_flip=True,\n       #brightness_range=[0.5, 1.5],\n       #fill_mode='reflect',                               \n        rotation_range=15,\n        rescale=1./255,\n        #shear_range=0.2,\n        #zoom_range=0.2\n        validation_split=0.10\n    \n)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_path = '../input/histopathologic-cancer-detection/train'\nvalid_path = '../input/histopathologic-cancer-detection/train'\n\ntrain_generator = train_datagen.flow_from_dataframe(\n                dataframe=df,\n                directory=train_path,\n                x_col = 'id',\n                y_col = 'label',\n#                 has_ext=True,\n                subset='training',\n                target_size=(96, 96),\n                batch_size=64,\n                class_mode='binary'\n                )\n\nvalidation_generator = train_datagen.flow_from_dataframe(\n                dataframe=df,\n                directory=valid_path,\n                x_col = 'id',\n                y_col = 'label',\n#                 has_ext=True,\n                subset='validation', # This is the trick to properly separate train and validation dataset\n                target_size=(96, 96),\n                batch_size=64,\n                shuffle=False,\n                class_mode='binary'\n                )","metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:06:44.969944Z","iopub.execute_input":"2023-05-22T19:06:44.970600Z","iopub.status.idle":"2023-05-22T19:17:07.262177Z","shell.execute_reply.started":"2023-05-22T19:06:44.970567Z","shell.execute_reply":"2023-05-22T19:17:07.261257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Model Architecture","metadata":{}},{"cell_type":"markdown","source":"### Model 1","metadata":{}},{"cell_type":"code","source":"# with tf.device('/gpu:0'):  \n#     model = Sequential()\n    \n#     model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = (96, 96, 3)))\n#     model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n#     model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n#     model.add(Dropout(0.3))\n#     model.add(MaxPooling2D(pool_size = 3))\n\n#     model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n#     model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n#     model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n#     model.add(Dropout(0.3))\n#     model.add(MaxPooling2D(pool_size = 3))\n\n#     model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n#     model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n#     model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n#     model.add(Dropout(0.3))\n#     model.add(MaxPooling2D(pool_size = 3))\n\n#     model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\n#     model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\n#     model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\n#     model.add(Dropout(0.3))\n\n#     model.add(Flatten())\n#     model.add(Dense(128, activation='relu'))\n#     model.add(Dropout(0.3))\n#     model.add(Dense(1, activation = 'sigmoid'))\n# #     model.summary()\n\n#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n#     STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n#     STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n\n#     history = model.fit(train_generator,\n#                         steps_per_epoch=STEP_SIZE_TRAIN,\n#                         epochs=15,\n#                         validation_data=validation_generator,\n#                         validation_steps=STEP_SIZE_VALID)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:17:07.263726Z","iopub.execute_input":"2023-05-22T19:17:07.264076Z","iopub.status.idle":"2023-05-22T19:17:07.270444Z","shell.execute_reply.started":"2023-05-22T19:17:07.264044Z","shell.execute_reply":"2023-05-22T19:17:07.269487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model 2","metadata":{}},{"cell_type":"code","source":"with tf.device('/gpu:0'):  \n    model = Sequential()\n    \n    model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = (96, 96, 3)))\n    model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n    model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu'))\n    model.add(Dropout(0.3))\n    model.add(MaxPooling2D(pool_size = 3))\n\n    model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n    model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n    model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n    model.add(Dropout(0.3))\n    model.add(MaxPooling2D(pool_size = 3))\n\n    model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n    model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n    model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n    model.add(Dropout(0.3))\n    model.add(MaxPooling2D(pool_size = 3))\n\n    model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\n    model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\n    model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'elu'))\n    model.add(Dropout(0.3))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation = 'sigmoid'))\n#     model.summary()\n\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n    STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n\n    history = model.fit(train_generator,\n                        steps_per_epoch=STEP_SIZE_TRAIN,\n                        epochs=10,\n                        validation_data=validation_generator,\n                        validation_steps=STEP_SIZE_VALID)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T19:17:07.272037Z","iopub.execute_input":"2023-05-22T19:17:07.272366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# # model.compile(optimizer.rmsprop(lr=0.0001, decay=1e-6),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n\n# STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n# STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n\n\n# history = model.fit(train_generator,\n#                     steps_per_epoch=STEP_SIZE_TRAIN,\n#                     epochs=15,\n#                     validation_data=validation_generator,\n#                     validation_steps=STEP_SIZE_VALID)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate_generator(generator=validation_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nmodel.save('Keras_CNN_2.h5')  ","metadata":{"execution":{"iopub.status.busy":"2023-05-22T22:15:33.367085Z","iopub.execute_input":"2023-05-22T22:15:33.367431Z","iopub.status.idle":"2023-05-22T22:15:33.470113Z","shell.execute_reply.started":"2023-05-22T22:15:33.367404Z","shell.execute_reply":"2023-05-22T22:15:33.468961Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Returns a compiled model identical to the previous one\nmodel = load_model('/kaggle/working/Keras_CNN_2.h5')","metadata":{"execution":{"iopub.status.busy":"2023-05-22T22:15:19.922076Z","iopub.execute_input":"2023-05-22T22:15:19.922780Z","iopub.status.idle":"2023-05-22T22:15:20.280950Z","shell.execute_reply.started":"2023-05-22T22:15:19.922745Z","shell.execute_reply":"2023-05-22T22:15:20.279946Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(path+'sample_submission.csv')\n\nfrom matplotlib.pyplot import imread\n# Kaggle testing\nfrom glob import glob\nTESTING_BATCH_SIZE = 64\ntesting_files = glob(os.path.join('../input/test/','*.tif'))\nsubmission = pd.DataFrame()\nprint(len(testing_files))\nfor index in range(0, len(testing_files), TESTING_BATCH_SIZE):\n    data_frame = pd.DataFrame({'path': testing_files[index:index+TESTING_BATCH_SIZE]})\n    data_frame['id'] = data_frame.path.map(lambda x: x.split('/')[3].split(\".\")[0])\n    data_frame['image'] = data_frame['path'].map(imread)\n    images = np.stack(data_frame.image, axis=0)\n    predicted_labels = [model.predict(np.expand_dims(image/255.0, axis=0))[0][0] for image in images]\n    predictions = np.array(predicted_labels)\n    data_frame['label'] = predictions\n    submission = pd.concat([submission, data_frame[[\"id\", \"label\"]]])\n    if index % 1000 == 0 :\n        print(index/len(testing_files) * 100)\nsubmission.to_csv('submission_new_model.csv', index=False, header=True)\nprint(submission.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}